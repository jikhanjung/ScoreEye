### **Stage-1 훈련 실패 원인 분석 보고서**

**로그 파일**: `deepscores_workspace/deepscores-stage1.20250725.log`
**작성일**: 2025년 7월 25일

---

### **1. 현상 및 직접적인 원인**

- **현상**: 훈련이 29번째 Epoch에서 조기 종료되었으며, 최종 모델의 성능이 최고점(9 Epoch)에 비해 현저히 낮게 기록됨.
- **직접 원인**: **Loss 값 발산 (NaN 발생)**
  - 로그의 18번째 Epoch부터 `box_loss`, `cls_loss`, `dfl_loss`가 모두 `nan` (Not a Number)으로 기록되기 시작했습니다.
  - 이는 모델의 가중치를 업데이트하는 과정에서 계산된 손실 값이 무한대(infinity)로 폭발(exploding gradients)하여 더 이상 훈련을 진행할 수 없는 상태가 되었음을 의미합니다.

```log
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
     18/150      10.9G        nan        nan        nan        461       1536
```

### **2. 근본적인 원인 분석**

Loss 값이 발산하게 된 근본적인 원인은 다음과 같이 3가지로 추정됩니다.

#### **2.1. 가장 유력한 원인: 과도하게 높은 학습률 (Learning Rate)**

- **설정**: `lr0=0.01`
- **문제점**: `0.01`은 YOLOv8의 기본값이지만, 이는 일반적인 데이터셋 기준입니다. DeepScores 데이터셋은 다음과 같은 특성으로 인해 더 낮은 학습률을 요구합니다:
    1.  **고밀도 객체**: 악보는 작은 영역에 수많은 객체가 밀집해 있습니다.
    2.  **고해상도 입력**: `imgsz=1536` 으로 매우 큰 이미지를 사용하고 있어, 가중치 업데이트의 영향이 더 큽니다.
    3.  **클래스 불균형**: 특정 클래스가 데이터셋의 대부분을 차지합니다.
- **증거**: 
    - **성능 급락**: 9번째 Epoch에서 mAP@0.5가 0.764로 최고점을 기록한 후, 13번째 Epoch에서는 0.041로 수직 하락했습니다. 이는 높은 학습률로 인해 모델이 최적점을 크게 벗어나는(overshooting) 전형적인 현상입니다.
    - **W&B 로그**: `lr/pg0` 그래프가 초반에 매우 높은 값에서 시작하여 급격히 감소하는 패턴을 보입니다. 이 초기 충격이 학습 과정을 불안정하게 만들었을 가능성이 높습니다.

#### **2.2. 두 번째 원인: 부적절한 옵티마이저 설정**

- **설정**: `optimizer=AdamW`
- **문제점**: `AdamW`는 좋은 옵티마이저이지만, 특정 데이터셋과 높은 학습률이 결합될 경우 가중치 업데이트 폭이 너무 커져 불안정성을 유발할 수 있습니다. YOLOv8의 기본 옵티마이저 중 하나인 `SGD`가 더 안정적인 선택일 수 있습니다.

#### **2.3. 세 번째 원인: 데이터셋의 잠재적 노이즈**

- **로그 메시지**: `1 duplicate labels removed` 와 같은 경고가 학습 및 검증 데이터셋 모두에서 다수 발견되었습니다.
- **문제점**: 이는 라벨링 데이터의 품질이 완벽하지 않음을 시사합니다. 이러한 데이터 노이즈는 학습 과정을 미세하게 불안정하게 만드는 요인으로 작용할 수 있습니다.

#### **2.4. 네 번째 원인: GPU 메모리 초과 및 불안정**

- **현상**: 배치 크기 1임에도 GPU 메모리가 7.78GB에서 16GB까지 극심하게 변동
- **문제점**: 
  - RTX 2080 Ti (11GB) 한계 초과로 공유 메모리(시스템 RAM) 사용
  - 메모리 스왑으로 인한 성능 저하 및 계산 불안정성
  - GPU-RAM 간 데이터 전송 속도가 GPU-VRAM보다 20배 이상 느림
- **증거**: 
  - 에폭 13에서 15.4GB 기록
  - 에폭 27에서 16GB 기록 (11GB GPU에서 5GB 초과)
  - 메모리 초과 시점과 성능 급락 시점이 일치

#### **2.5. 다섯 번째 원인: 극도로 작은 배치 크기**

- **설정**: `batch=1`
- **문제점**: 
  - Gradient 노이즈가 매우 심함
  - 단일 이미지의 극단적 gradient가 전체 학습에 영향
  - 높은 학습률(0.01)과 결합시 불안정성 극대화
  - Gradient accumulation이 있어도 개별 배치의 극단값이 문제 유발 가능
- **증거**: 에폭별 성능 변동폭이 매우 큼 (mAP 0.764 → 0.576 → 0.041)

---

### **3. 해결 방안 및 추천 조치**

다음과 같은 순서로 조치를 취하는 것을 권장합니다.

1.  **학습률(Learning Rate) 대폭 하향 조정 (우선순위 높음)**:
    - **조치**: `lr0=0.01`을 **`lr0=0.001`** (기존의 1/10)로 낮춰서 학습을 재시도합니다.
    - **기대 효과**: 이것만으로도 Loss 발산 문제가 해결될 가능성이 매우 높습니다.

2.  **옵티마이저(Optimizer) 변경**:
    - **조치**: `optimizer=AdamW` 대신 **`optimizer=SGD`** 로 변경하여 안정성을 높입니다.

3.  **Warmup Epoch 증가**:
    - **조치**: `warmup_epochs=3`을 **`warmup_epochs=5`** 로 늘립니다.
    - **기대 효과**: 본격적인 학습 시작 전, 모델이 매우 낮은 학습률로 안정적으로 준비되는 기간을 늘려 초기 불안정성을 억제합니다.

4.  **데이터 클리닝 (장기적 관점)**:
    - **조치**: `Phase 2: 데이터 전처리 및 품질 검증` 계획에 따라, 중복 라벨 제거를 포함한 데이터셋 검증 및 정제 파이프라인을 강화합니다.

5.  **배치 크기 증가 및 이미지 크기 조정**:
    - **조치**: 메모리 제약을 고려하여 다음 중 하나 선택
      - `imgsz=1280` + `batch=3` + `mosaic=0`
      - `imgsz=1024` + `batch=4` + `mosaic=0`
    - **기대 효과**: 메모리 사용량 안정화 및 gradient 안정성 향상

6.  **Gradient Clipping 활성화**:
    - **조치**: YOLOv8 훈련 설정에 gradient clipping 추가 (max_norm=10.0)
    - **기대 효과**: 극단적인 gradient 값 제한으로 NaN 방지

7.  **Mixed Precision 비활성화 (임시)**:
    - **조치**: `--no-amp` 옵션 사용
    - **기대 효과**: FP16 연산의 수치적 불안정성 제거

### **4. 클래스별 성능 불균형 분석**

최고 성능을 기록한 에폭 9에서도 특정 클래스들의 성능이 매우 낮았습니다:

- **stem**: mAP = 0 (완전 실패)
  - 매우 얇은 수직선으로 검출이 극도로 어려움
  - 1536×1536 해상도에서도 1-2픽셀 너비로 표현됨
- **ledgerLine**: Recall = 0.0011 (거의 검출 안됨)
  - 짧고 얇은 수평선으로 stem과 유사한 문제
- **augmentationDot**: Recall = 0.182 (매우 낮음)
  - 작은 점 형태로 크기가 매우 작음
- **fermataAbove**: mAP = 0 (완전 실패)
  - 복잡한 곡선 형태로 학습이 어려움

이들은 모두 매우 얇거나 작은 객체로, 현재 해상도와 모델 구조로는 검출이 근본적으로 어려운 것으로 보입니다.

### **5. 권장 재훈련 명령어**

위 분석을 바탕으로 다음 명령어로 재훈련을 시작하는 것을 권장합니다:

```bash
# 옵션 1: 안정성 최우선 (권장)
python train_stage1.py --lr0 0.001 --batch-size 2 --mosaic 0 --epochs 100

# 옵션 2: 이미지 크기 조정으로 메모리 확보
python train_stage1.py --lr0 0.001 --imgsz 1280 --batch-size 3 --mosaic 0 --epochs 100

# 옵션 3: 더 작은 모델 사용
python train_stage1.py --model-size yolov8n.pt --lr0 0.001 --batch-size 4 --mosaic 0 --epochs 100
```

### **요약**

**훈련 실패의 핵심 원인은 DeepScores 데이터셋의 특성을 고려하지 않은 과도하게 높은 학습률(`lr0=0.01`)과 GPU 메모리 초과로 인한 불안정성입니다. 학습률을 1/10로 낮추고, 배치 크기와 이미지 크기를 적절히 조정하여 메모리 사용을 11GB 이내로 유지하는 것이 필수적입니다.**
