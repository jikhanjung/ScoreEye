#!/usr/bin/env python3
"""
DeepScores ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
NPZ ì–´ë…¸í…Œì´ì…˜ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import random
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, Counter

import numpy as np
import cv2
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import matplotlib.patches as patches

class DeepScoresPreprocessor:
    """DeepScores ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, raw_data_dir: str, output_dir: str, pilot_mode: bool = False, sample_size: int = 1000):
        self.raw_data_dir = Path(raw_data_dir)
        self.output_dir = Path(output_dir)
        self.pilot_mode = pilot_mode
        self.sample_size = sample_size
        
        # ëª©í‘œ í´ë˜ìŠ¤ ì •ì˜ (ì ì§„ì  í™•ì¥ì„ ìœ„í•´ Phaseë³„ë¡œ ë¶„ë¥˜)
        self.target_classes_phases = {
            'phase_1': ['noteheadFull', 'stem', 'gClef'],
            'phase_2': ['restQuarter', 'beam', 'dot'],
            'phase_3': ['sharp', 'flat', 'natural'],
            'phase_4': ['timeSig4_4', 'keySigFlat1', 'noteheadHalf']
        }
        
        # ì „ì²´ ëª©í‘œ í´ë˜ìŠ¤ (ì´ˆê¸°ì—ëŠ” Phase 1-2ë§Œ ì‚¬ìš©)
        self.target_classes = {}
        class_id = 0
        for phase_classes in self.target_classes_phases.values():
            for class_name in phase_classes:
                self.target_classes[class_name] = class_id
                class_id += 1
        
        self.image_size = 1024  # DeepScores ê³ í•´ìƒë„ ì²˜ë¦¬
        self.train_ratio = 0.85
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.validation_results = {}
        
    def convert_to_yolo(self, bbox: List[float], img_width: int, img_height: int) -> Tuple[float, float, float, float]:
        """DeepScores bbox [x_min, y_min, x_max, y_max] -> YOLO í¬ë§· ë³€í™˜"""
        dw = 1.0 / img_width
        dh = 1.0 / img_height
        
        x_center = (bbox[0] + bbox[2]) / 2.0
        y_center = (bbox[1] + bbox[3]) / 2.0
        width = bbox[2] - bbox[0]
        height = bbox[3] - bbox[1]
        
        return (x_center * dw, y_center * dh, width * dw, height * dh)
    
    def validate_bbox(self, bbox: Tuple[float, float, float, float]) -> bool:
        """YOLO ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬"""
        x_center, y_center, width, height = bbox
        
        # 0-1 ë²”ìœ„ í™•ì¸
        if not (0 <= x_center <= 1 and 0 <= y_center <= 1):
            return False
        if not (0 < width <= 1 and 0 < height <= 1):
            return False
            
        # ê²½ê³„ í™•ì¸
        if (x_center - width/2 < 0) or (x_center + width/2 > 1):
            return False
        if (y_center - height/2 < 0) or (y_center + height/2 > 1):
            return False
            
        return True
    
    def smart_dataset_sampling(self, all_images: List[str]) -> List[str]:
        """í´ë˜ìŠ¤ ê· í˜•ì„ ê³ ë ¤í•œ ì§€ëŠ¥ì  ë°ì´í„° ìƒ˜í”Œë§"""
        if not self.pilot_mode:
            return all_images
        
        print(f"ğŸ¯ Pilot Mode: {self.sample_size}ê°œ ìƒ˜í”Œ ì„ íƒ ì¤‘...")
        
        # ê° ì´ë¯¸ì§€ì˜ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
        image_class_counts = {}
        target_class_names = set(self.target_classes.keys())
        
        for img_name in tqdm(all_images[:5000], desc="í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"):  # ìµœëŒ€ 5000ê°œë§Œ ë¶„ì„
            npz_path = self.raw_data_dir / "annotations" / (img_name.replace('.png', '.npz'))
            if not npz_path.exists():
                continue
                
            try:
                annotations = np.load(npz_path, allow_pickle=True)['arr_0']
                class_counts = Counter()
                
                for ann in annotations:
                    if hasattr(ann, 'item'):
                        ann = ann.item()
                    class_name = ann.get('class_name', '')
                    
                    if class_name in target_class_names:
                        class_counts[class_name] += 1
                
                image_class_counts[img_name] = class_counts
                
            except Exception as e:
                print(f"Warning: íŒŒì¼ {npz_path} ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
        
        # ê· í˜•ì¡íŒ ìƒ˜í”Œë§
        sampled_images = []
        class_samples = defaultdict(list)
        
        # í´ë˜ìŠ¤ë³„ë¡œ ì´ë¯¸ì§€ ê·¸ë£¹í•‘
        for img_name, class_counts in image_class_counts.items():
            for class_name in class_counts.keys():
                class_samples[class_name].append((img_name, class_counts[class_name]))
        
        # ê° í´ë˜ìŠ¤ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
        samples_per_class = max(50, self.sample_size // len(self.target_classes))  # í´ë˜ìŠ¤ë‹¹ ìµœì†Œ 50ê°œ
        
        for class_name, images in class_samples.items():
            # í•´ë‹¹ í´ë˜ìŠ¤ê°€ ë§ì´ í¬í•¨ëœ ì´ë¯¸ì§€ ìš°ì„  ì„ íƒ
            images.sort(key=lambda x: x[1], reverse=True)
            selected = [img[0] for img in images[:samples_per_class]]
            sampled_images.extend(selected)
        
        # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ìƒ˜í”Œ ìˆ˜ ì¡°ì •
        sampled_images = list(set(sampled_images))
        
        if len(sampled_images) > self.sample_size:
            sampled_images = random.sample(sampled_images, self.sample_size)
        
        print(f"âœ… {len(sampled_images)}ê°œ ìƒ˜í”Œ ì„ íƒ ì™„ë£Œ")
        return sampled_images
    
    def process_dataset(self):
        """ë©”ì¸ ë°ì´í„°ì…‹ ì²˜ë¦¬ í•¨ìˆ˜"""
        print("ğŸš€ DeepScores ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘")
        print(f"ì›ë³¸ ë°ì´í„°: {self.raw_data_dir}")
        print(f"ì¶œë ¥ ë””ë ‰í„°ë¦¬: {self.output_dir}")
        print(f"Pilot Mode: {self.pilot_mode} (ìƒ˜í”Œ í¬ê¸°: {self.sample_size})")
        print("=" * 60)
        
        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        for subset in ['train', 'val']:
            (self.output_dir / 'images' / subset).mkdir(parents=True, exist_ok=True)
            (self.output_dir / 'labels' / subset).mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        images_dir = self.raw_data_dir / "images_png"
        if not images_dir.exists():
            images_dir = self.raw_data_dir / "images"  # ëŒ€ì²´ ê²½ë¡œ
            
        if not images_dir.exists():
            raise FileNotFoundError(f"ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {images_dir}")
        
        all_images = [f.name for f in images_dir.glob('*.png')]
        print(f"ğŸ“ ì´ ì´ë¯¸ì§€ ìˆ˜: {len(all_images)}")
        
        if len(all_images) == 0:
            raise ValueError("ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§
        selected_images = self.smart_dataset_sampling(all_images)
        
        # Train/Validation ë¶„í• 
        train_images, val_images = train_test_split(
            selected_images, 
            train_size=self.train_ratio, 
            random_state=42
        )
        
        print(f"ğŸ“Š Train: {len(train_images)}, Validation: {len(val_images)}")
        
        # ë°ì´í„°ì…‹ ì²˜ë¦¬
        processing_stats = {
            'total_images': 0,
            'processed_images': 0,
            'total_annotations': 0,
            'valid_annotations': 0,
            'invalid_bboxes': 0,
            'missing_files': 0,
            'class_distribution': Counter()
        }
        
        for subset, image_list in [('train', train_images), ('val', val_images)]:
            print(f"\nğŸ”„ {subset.upper()} ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
            
            img_out_path = self.output_dir / 'images' / subset
            lbl_out_path = self.output_dir / 'labels' / subset
            
            for img_name in tqdm(image_list, desc=f"Processing {subset}"):
                processing_stats['total_images'] += 1
                
                try:
                    # ì›ë³¸ ì´ë¯¸ì§€ ì²˜ë¦¬
                    img_path = images_dir / img_name
                    if not img_path.exists():
                        processing_stats['missing_files'] += 1
                        continue
                    
                    original_img = cv2.imread(str(img_path))
                    if original_img is None:
                        processing_stats['missing_files'] += 1
                        continue
                        
                    h, w, _ = original_img.shape
                    
                    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë° ì €ì¥
                    resized_img = cv2.resize(original_img, (self.image_size, self.image_size))
                    cv2.imwrite(str(img_out_path / img_name), resized_img)
                    
                    # NPZ ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ
                    npz_path = self.raw_data_dir / "annotations" / img_name.replace('.png', '.npz')
                    if not npz_path.exists():
                        # ë¹ˆ ë¼ë²¨ íŒŒì¼ ìƒì„±
                        (lbl_out_path / img_name.replace('.png', '.txt')).touch()
                        processing_stats['missing_files'] += 1
                        continue
                    
                    annotations = np.load(npz_path, allow_pickle=True)['arr_0']
                    
                    yolo_labels = []
                    for ann in annotations:
                        processing_stats['total_annotations'] += 1
                        
                        if hasattr(ann, 'item'):
                            ann = ann.item()
                        
                        class_name = ann.get('class_name', '')
                        if class_name not in self.target_classes:
                            continue
                        
                        class_id = self.target_classes[class_name]
                        bbox = ann.get('bbox', [])
                        
                        if len(bbox) != 4:
                            processing_stats['invalid_bboxes'] += 1
                            continue
                        
                        # YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜
                        yolo_bbox = self.convert_to_yolo(bbox, w, h)
                        
                        # ë°”ìš´ë”© ë°•ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
                        if not self.validate_bbox(yolo_bbox):
                            processing_stats['invalid_bboxes'] += 1
                            continue
                        
                        yolo_labels.append(f"{class_id} {' '.join(map(str, yolo_bbox))}")
                        processing_stats['valid_annotations'] += 1
                        processing_stats['class_distribution'][class_name] += 1
                    
                    # ë¼ë²¨ íŒŒì¼ ì €ì¥
                    label_file = lbl_out_path / img_name.replace('.png', '.txt')
                    with open(label_file, 'w') as f:
                        f.write('\n'.join(yolo_labels))
                    
                    processing_stats['processed_images'] += 1
                    
                except Exception as e:
                    print(f"ì˜¤ë¥˜: {img_name} ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")
                    continue
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        self._print_processing_summary(processing_stats)
        
        # YAML ì„¤ì • íŒŒì¼ ìƒì„±
        self._create_dataset_yaml()
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
        self._validate_dataset_quality(processing_stats)
        
        print("\nğŸ‰ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    
    def _print_processing_summary(self, stats: Dict):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ì´ ì´ë¯¸ì§€ ìˆ˜: {stats['total_images']}")
        print(f"ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {stats['processed_images']}")
        print(f"ì´ ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {stats['total_annotations']}")
        print(f"ìœ íš¨í•œ ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {stats['valid_annotations']}")
        print(f"ë¬´íš¨í•œ ë°”ìš´ë”© ë°•ìŠ¤: {stats['invalid_bboxes']}")
        print(f"ëˆ„ë½ëœ íŒŒì¼: {stats['missing_files']}")
        
        print(f"\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        for class_name, count in stats['class_distribution'].most_common():
            percentage = (count / stats['valid_annotations']) * 100 if stats['valid_annotations'] > 0 else 0
            print(f"  {class_name}: {count} ({percentage:.1f}%)")
    
    def _create_dataset_yaml(self):
        """YAML ì„¤ì • íŒŒì¼ ìƒì„±"""
        yaml_content = f"""# DeepScores Dataset Configuration for YOLOv8
path: {self.output_dir.absolute()}
train: images/train
val: images/val

# Classes ({len(self.target_classes)} classes)
names:
"""
        
        for class_name, class_id in sorted(self.target_classes.items(), key=lambda x: x[1]):
            yaml_content += f"  {class_id}: {class_name}\n"
        
        yaml_file = self.output_dir / "deepscores.yaml"
        with open(yaml_file, 'w') as f:
            f.write(yaml_content)
        
        print(f"ğŸ“„ YAML ì„¤ì • íŒŒì¼ ìƒì„±: {yaml_file}")
    
    def _validate_dataset_quality(self, processing_stats: Dict):
        """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ” ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        validation_results = {
            'processing_stats': processing_stats,
            'image_label_pairs': self._check_image_label_pairs(),
            'class_imbalance': self._analyze_class_imbalance(processing_stats['class_distribution']),
            'sample_visualization': self._create_sample_visualization()
        }
        
        # ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥
        report_dir = Path("validation_reports")
        report_dir.mkdir(exist_ok=True)
        
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = report_dir / f"preprocessing_validation_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            # Counter ê°ì²´ë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ í•¨
            serializable_results = validation_results.copy()
            serializable_results['processing_stats']['class_distribution'] = dict(processing_stats['class_distribution'])
            json.dump(serializable_results, f, indent=2)
        
        print(f"ğŸ“‹ ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        
        self.validation_results = validation_results
    
    def _check_image_label_pairs(self) -> Dict:
        """ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ë§¤ì¹­ í™•ì¸"""
        results = {'train': {}, 'val': {}}
        
        for subset in ['train', 'val']:
            img_dir = self.output_dir / 'images' / subset
            lbl_dir = self.output_dir / 'labels' / subset
            
            img_files = set(f.stem for f in img_dir.glob('*.png'))
            lbl_files = set(f.stem for f in lbl_dir.glob('*.txt'))
            
            results[subset] = {
                'total_images': len(img_files),
                'total_labels': len(lbl_files),
                'missing_labels': len(img_files - lbl_files),
                'orphan_labels': len(lbl_files - img_files),
                'matched_pairs': len(img_files & lbl_files)
            }
        
        return results
    
    def _analyze_class_imbalance(self, class_distribution: Counter) -> Dict:
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„"""
        if not class_distribution:
            return {'status': 'no_data'}
        
        total = sum(class_distribution.values())
        class_ratios = {cls: count/total for cls, count in class_distribution.items()}
        
        # ë¶ˆê· í˜• ì‹¬ê°ë„ ê³„ì‚°
        max_ratio = max(class_ratios.values())
        min_ratio = min(class_ratios.values())
        imbalance_ratio = max_ratio / min_ratio if min_ratio > 0 else float('inf')
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        if imbalance_ratio > 50:
            recommendations.append("ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• - weighted loss í•¨ìˆ˜ ì‚¬ìš© ê¶Œì¥")
        if imbalance_ratio > 20:
            recommendations.append("í´ë˜ìŠ¤ë³„ ë°ì´í„° augmentation ì°¨ë“± ì ìš© ê¶Œì¥")
        if min_ratio < 0.01:  # 1% ë¯¸ë§Œ
            recommendations.append("í¬ê·€ í´ë˜ìŠ¤ì— ëŒ€í•œ oversampling ê¶Œì¥")
        
        return {
            'imbalance_ratio': imbalance_ratio,
            'class_ratios': class_ratios,
            'recommendations': recommendations
        }
    
    def _create_sample_visualization(self, num_samples: int = 10) -> Dict:
        """ìƒ˜í”Œ ì´ë¯¸ì§€ ì‹œê°í™”"""
        print("ğŸ“¸ ìƒ˜í”Œ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        try:
            train_img_dir = self.output_dir / 'images' / 'train'
            train_lbl_dir = self.output_dir / 'labels' / 'train'
            
            img_files = list(train_img_dir.glob('*.png'))
            if len(img_files) == 0:
                return {'status': 'no_images'}
            
            sample_files = random.sample(img_files, min(num_samples, len(img_files)))
            
            # ì‹œê°í™” ìƒì„±
            fig, axes = plt.subplots(2, 5, figsize=(20, 8))
            axes = axes.flatten()
            
            visualized_count = 0
            
            for i, img_file in enumerate(sample_files):
                if i >= len(axes):
                    break
                
                # ì´ë¯¸ì§€ ë¡œë“œ
                image = cv2.imread(str(img_file))
                if image is None:
                    continue
                    
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                # ë¼ë²¨ ë¡œë“œ
                label_file = train_lbl_dir / (img_file.stem + '.txt')
                
                ax = axes[i]
                ax.imshow(image_rgb)
                ax.set_title(f"{img_file.stem}")
                ax.axis('off')
                
                if label_file.exists():
                    with open(label_file, 'r') as f:
                        lines = f.read().strip().split('\n')
                        
                    h, w = image_rgb.shape[:2]
                    
                    for line in lines:
                        if not line.strip():
                            continue
                        parts = line.split()
                        if len(parts) != 5:
                            continue
                        
                        class_id, x_center, y_center, width, height = map(float, parts)
                        
                        # YOLO -> í”½ì…€ ì¢Œí‘œ ë³€í™˜
                        x1 = (x_center - width/2) * w
                        y1 = (y_center - height/2) * h
                        box_width = width * w
                        box_height = height * h
                        
                        rect = patches.Rectangle(
                            (x1, y1), box_width, box_height,
                            linewidth=2, edgecolor='red', facecolor='none'
                        )
                        ax.add_patch(rect)
                        
                        # í´ë˜ìŠ¤ ì´ë¦„ í‘œì‹œ
                        class_name = [name for name, id in self.target_classes.items() if id == int(class_id)]
                        class_name = class_name[0] if class_name else f"Class_{int(class_id)}"
                        
                        ax.text(x1, y1-5, class_name, color='red', fontsize=8, 
                               bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.7))
                
                visualized_count += 1
            
            # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
            for i in range(visualized_count, len(axes)):
                axes[i].axis('off')
            
            plt.tight_layout()
            
            # ì‹œê°í™” ì €ì¥
            vis_dir = Path("validation_reports")
            vis_dir.mkdir(exist_ok=True)
            
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            vis_file = vis_dir / f"sample_visualization_{timestamp}.png"
            
            plt.savefig(vis_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            return {
                'status': 'success',
                'visualized_samples': visualized_count,
                'output_file': str(vis_file)
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'error': str(e)
            }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="DeepScores ë°ì´í„°ì…‹ ì „ì²˜ë¦¬")
    parser.add_argument("--raw-data", default="raw_data", help="ì›ë³¸ DeepScores ë°ì´í„° ë””ë ‰í„°ë¦¬")
    parser.add_argument("--output", default="data", help="ì¶œë ¥ ë””ë ‰í„°ë¦¬")
    parser.add_argument("--pilot-mode", action="store_true", help="Pilot ëª¨ë“œ (ìƒ˜í”Œ ë°ì´í„°ë§Œ ì²˜ë¦¬)")
    parser.add_argument("--sample-size", type=int, default=1000, help="Pilot ëª¨ë“œ ìƒ˜í”Œ í¬ê¸°")
    
    args = parser.parse_args()
    
    processor = DeepScoresPreprocessor(
        raw_data_dir=args.raw_data,
        output_dir=args.output,
        pilot_mode=args.pilot_mode,
        sample_size=args.sample_size
    )
    
    try:
        processor.process_dataset()
        print("âœ… ì „ì²˜ë¦¬ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()