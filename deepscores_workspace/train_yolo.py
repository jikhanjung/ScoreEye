#!/usr/bin/env python3
"""
YOLOv8 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ with ê³ ê¸‰ ê¸°ëŠ¥
ì ì§„ì  í´ë˜ìŠ¤ í™•ì¥, ëª¨ë‹ˆí„°ë§, ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import yaml

import numpy as np
import torch
from ultralytics import YOLO
import wandb
import optuna


class YOLOTrainer:
    """ê³ ê¸‰ ê¸°ëŠ¥ì„ í¬í•¨í•œ YOLO í•™ìŠµ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 data_yaml: str,
                 model_size: str = 'yolov8s',
                 project_name: str = 'scoreeye-yolov8',
                 use_wandb: bool = True):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_yaml: ë°ì´í„°ì…‹ YAML íŒŒì¼ ê²½ë¡œ
            model_size: ëª¨ë¸ í¬ê¸° ('yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x')
            project_name: í”„ë¡œì íŠ¸ ì´ë¦„
            use_wandb: Weights & Biases ì‚¬ìš© ì—¬ë¶€
        """
        self.data_yaml = Path(data_yaml)
        self.model_size = model_size
        self.project_name = project_name
        self.use_wandb = use_wandb
        
        if not self.data_yaml.exists():
            raise FileNotFoundError(f"ë°ì´í„° YAML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_yaml}")
        
        # ë°ì´í„° ì •ë³´ ë¡œë“œ
        with open(self.data_yaml, 'r') as f:
            self.data_config = yaml.safe_load(f)
        
        self.num_classes = len(self.data_config.get('names', {}))
        print(f"ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {self.num_classes}")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = self._get_optimal_device()
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # W&B ì´ˆê¸°í™”
        if self.use_wandb:
            self._initialize_wandb()
    
    def _get_optimal_device(self) -> str:
        """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
            return 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'
        else:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
            return 'cpu'
    
    def _initialize_wandb(self):
        """Weights & Biases ì´ˆê¸°í™”"""
        try:
            wandb.init(
                project=self.project_name,
                config={
                    "model": self.model_size,
                    "dataset": str(self.data_yaml),
                    "num_classes": self.num_classes,
                    "device": self.device,
                    "timestamp": datetime.now().isoformat()
                }
            )
            print("ğŸ“ˆ W&B ì—°ë™ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ W&B ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.use_wandb = False
    
    def progressive_training(self, phases: Dict[str, Dict]) -> Dict[str, str]:
        """
        ì ì§„ì  í´ë˜ìŠ¤ í™•ì¥ í•™ìŠµ
        
        Args:
            phases: ê° ë‹¨ê³„ë³„ ì„¤ì •
            {
                'phase_1': {
                    'classes': ['noteheadFull', 'stem', 'gClef'],
                    'epochs': 50,
                    'target_mAP': 0.85
                },
                ...
            }
        
        Returns:
            ê° ë‹¨ê³„ë³„ ìµœì¢… ëª¨ë¸ ê²½ë¡œ
        """
        print("ğŸ¯ ì ì§„ì  í´ë˜ìŠ¤ í™•ì¥ í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        
        phase_models = {}
        previous_model = f"{self.model_size}.pt"
        
        for phase_name, phase_config in phases.items():
            print(f"\nğŸš€ {phase_name} ì‹œì‘")
            print(f"   íƒ€ê²Ÿ í´ë˜ìŠ¤: {phase_config['classes']}")
            print(f"   ëª©í‘œ mAP: {phase_config.get('target_mAP', 'N/A')}")
            
            # í•´ë‹¹ ë‹¨ê³„ í´ë˜ìŠ¤ë§Œìœ¼ë¡œ ì œí•œëœ ë°ì´í„°ì…‹ ìƒì„±
            phase_yaml = self._create_phase_dataset(phase_name, phase_config['classes'])
            
            # í•™ìŠµ ì‹¤í–‰
            best_model = self.train(
                model_path=previous_model,
                data_yaml=phase_yaml,
                epochs=phase_config.get('epochs', 50),
                batch=phase_config.get('batch', 'auto'),
                imgsz=phase_config.get('imgsz', 2048),
                patience=phase_config.get('patience', 15),
                name=f"{phase_name}",
                save_period=phase_config.get('save_period', 10)
            )
            
            phase_models[phase_name] = best_model
            
            # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì´ ëª¨ë¸ì„ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©
            previous_model = best_model
            
            # ëª©í‘œ mAP ë‹¬ì„± ì—¬ë¶€ í™•ì¸
            if 'target_mAP' in phase_config:
                current_mAP = self._evaluate_model(best_model, phase_yaml)
                if current_mAP < phase_config['target_mAP']:
                    print(f"âš ï¸ ê²½ê³ : ëª©í‘œ mAP {phase_config['target_mAP']:.3f}ì— ë¯¸ë‹¬ (í˜„ì¬: {current_mAP:.3f})")
                else:
                    print(f"âœ… ëª©í‘œ ë‹¬ì„±: mAP {current_mAP:.3f}")
        
        print("\nğŸ‰ ì ì§„ì  í•™ìŠµ ì™„ë£Œ!")
        return phase_models
    
    def _create_phase_dataset(self, phase_name: str, target_classes: List[str]) -> str:
        """íŠ¹ì • í´ë˜ìŠ¤ë§Œ í¬í•¨í•˜ëŠ” ë°ì´í„°ì…‹ YAML ìƒì„±"""
        
        # ì›ë³¸ í´ë˜ìŠ¤ ì´ë¦„ì—ì„œ íƒ€ê²Ÿ í´ë˜ìŠ¤ì˜ ID ì°¾ê¸°
        class_mapping = {}
        new_id = 0
        
        for old_id, class_name in self.data_config['names'].items():
            if class_name in target_classes:
                class_mapping[int(old_id)] = new_id
                new_id += 1
        
        # ìƒˆ YAML ì„¤ì •
        phase_config = {
            'path': self.data_config['path'],
            'train': self.data_config['train'],
            'val': self.data_config['val'],
            'names': {new_id: name for old_id, new_id in class_mapping.items() 
                     for name in [self.data_config['names'][old_id]]}
        }
        
        # íŒŒì¼ ì €ì¥
        phase_yaml_path = self.data_yaml.parent / f"{phase_name}_dataset.yaml"
        with open(phase_yaml_path, 'w') as f:
            yaml.dump(phase_config, f, default_flow_style=False)
        
        print(f"ğŸ“„ {phase_name} ë°ì´í„°ì…‹ ìƒì„±: {phase_yaml_path}")
        print(f"   í¬í•¨ëœ í´ë˜ìŠ¤: {list(phase_config['names'].values())}")
        
        # ë¼ë²¨ íŒŒì¼ í•„í„°ë§ (í•´ë‹¹ í´ë˜ìŠ¤ë§Œ ë‚¨ê¸°ê³  ID ì¬ë§¤í•‘)
        self._filter_labels_for_phase(class_mapping, phase_name)
        
        return str(phase_yaml_path)
    
    def _filter_labels_for_phase(self, class_mapping: Dict[int, int], phase_name: str):
        """íŠ¹ì • ë‹¨ê³„ì— ë§ê²Œ ë¼ë²¨ íŒŒì¼ë“¤ì„ í•„í„°ë§í•˜ê³  ID ì¬ë§¤í•‘"""
        
        data_path = Path(self.data_config['path'])
        
        for subset in ['train', 'val']:
            labels_dir = data_path / 'labels' / subset
            phase_labels_dir = data_path / 'labels' / f"{subset}_{phase_name}"
            phase_labels_dir.mkdir(exist_ok=True)
            
            processed_files = 0
            
            for label_file in labels_dir.glob('*.txt'):
                new_lines = []
                
                with open(label_file, 'r') as f:
                    for line in f:
                        if not line.strip():
                            continue
                        
                        parts = line.strip().split()
                        if len(parts) != 5:
                            continue
                        
                        old_class_id = int(parts[0])
                        
                        # í•´ë‹¹ í´ë˜ìŠ¤ê°€ í˜„ì¬ ë‹¨ê³„ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                        if old_class_id in class_mapping:
                            new_class_id = class_mapping[old_class_id]
                            parts[0] = str(new_class_id)
                            new_lines.append(' '.join(parts))
                
                # ìƒˆ ë¼ë²¨ íŒŒì¼ ì €ì¥ (í•´ë‹¹ í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
                if new_lines:
                    new_label_file = phase_labels_dir / label_file.name
                    with open(new_label_file, 'w') as f:
                        f.write('\n'.join(new_lines))
                    processed_files += 1
            
            print(f"   {subset} ë¼ë²¨ í•„í„°ë§: {processed_files}ê°œ íŒŒì¼ ì²˜ë¦¬")
        
        # ì›ë³¸ YAMLì—ì„œ ìƒˆ ë¼ë²¨ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
        phase_yaml_path = self.data_yaml.parent / f"{phase_name}_dataset.yaml"
        with open(phase_yaml_path, 'r') as f:
            config = yaml.safe_load(f)
        
        config['train'] = f"images/train"  # ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
        config['val'] = f"images/val"
        
        with open(phase_yaml_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
    
    def train(self, 
              model_path: str = None,
              data_yaml: str = None,
              epochs: int = 100,
              batch: str = 'auto',
              imgsz: int = 2048,
              patience: int = 15,
              name: str = None,
              save_period: int = 10) -> str:
        """
        YOLO ëª¨ë¸ í•™ìŠµ
        
        Returns:
            ìµœì¢… ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        """
        
        if model_path is None:
            model_path = f"{self.model_size}.pt"
        
        if data_yaml is None:
            data_yaml = str(self.data_yaml)
        
        print(f"ğŸš€ í•™ìŠµ ì‹œì‘")
        print(f"   ëª¨ë¸: {model_path}")
        print(f"   ë°ì´í„°: {data_yaml}")
        print(f"   Epochs: {epochs}")
        print(f"   Batch: {batch}")
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {imgsz}")
        
        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        if batch == 'auto':
            batch = self._get_optimal_batch_size(imgsz)
            print(f"   ìë™ ë°°ì¹˜ í¬ê¸°: {batch}")
        
        # ëª¨ë¸ ë¡œë“œ
        model = YOLO(model_path)
        
        # í•™ìŠµ ì‹¤í–‰ (W&B í™œì„±í™”)
        import os
        os.environ['WANDB_PROJECT'] = self.project_name
        
        results = model.train(
            data=data_yaml,
            epochs=epochs,
            batch=batch,
            imgsz=imgsz,
            patience=patience,
            device=self.device,
            amp=True,  # Mixed precision training
            name=name or f"train_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            save_period=save_period,
            plots=True,
            verbose=True,
            # W&B í™œì„±í™” íŒŒë¼ë¯¸í„° ì¶”ê°€
            project=self.project_name if self.use_wandb else None
        )
        
        # W&Bì— ìµœì¢… ë©”íŠ¸ë¦­ ê¸°ë¡
        if self.use_wandb and hasattr(results, 'results_dict'):
            final_metrics = {
                'final_mAP50': results.results_dict.get('metrics/mAP50(B)', 0.0),
                'final_mAP50_95': results.results_dict.get('metrics/mAP50-95(B)', 0.0),
                'final_precision': results.results_dict.get('metrics/precision(B)', 0.0),
                'final_recall': results.results_dict.get('metrics/recall(B)', 0.0),
                'total_epochs': epochs,
                'final_box_loss': results.results_dict.get('train/box_loss', 0.0),
                'final_cls_loss': results.results_dict.get('train/cls_loss', 0.0)
            }
            wandb.log(final_metrics)
        
        # ìµœì¢… ëª¨ë¸ ê²½ë¡œ
        best_model_path = results.save_dir / 'weights' / 'best.pt'
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ: {best_model_path}")
        
        if self.use_wandb:
            wandb.log({"training_completed": True})
        
        return str(best_model_path)
    
    def _get_optimal_batch_size(self, imgsz: int) -> int:
        """ì´ë¯¸ì§€ í¬ê¸°ì™€ GPU ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•œ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if self.device == 'cpu':
            return 4
        
        try:
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                
                # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± (ì´ë¯¸ì§€ í¬ê¸°ì™€ GPU ë©”ëª¨ë¦¬ ê¸°ë°˜)
                if imgsz <= 640:
                    if gpu_memory_gb >= 16:
                        return 16
                    elif gpu_memory_gb >= 8:
                        return 12
                    else:
                        return 8
                elif imgsz <= 2048:
                    if gpu_memory_gb >= 16:
                        return 8
                    elif gpu_memory_gb >= 8:
                        return 4
                    else:
                        return 2
                else:  # > 1024
                    if gpu_memory_gb >= 16:
                        return 4
                    else:
                        return 2
            else:
                return 4
                
        except Exception:
            return 4
    
    def optimize_hyperparameters(self, 
                                n_trials: int = 20, 
                                timeout: int = 7200) -> Dict:
        """
        Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™”
        
        Args:
            n_trials: ì‹œí–‰ íšŸìˆ˜
            timeout: ìµœëŒ€ ì‹œê°„ (ì´ˆ)
            
        Returns:
            ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        """
        print(f"ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ìµœëŒ€ {n_trials}íšŒ, {timeout/60:.0f}ë¶„)")
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
            params = {
                'lr0': trial.suggest_loguniform('lr0', 1e-5, 1e-2),
                'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),
                'momentum': trial.suggest_uniform('momentum', 0.8, 0.99),
                'batch': trial.suggest_categorical('batch', [4, 8, 16]),
                'warmup_epochs': trial.suggest_int('warmup_epochs', 0, 5)
            }
            
            # ì„ì‹œ ëª¨ë¸ë¡œ ì§§ì€ í•™ìŠµ ì‹¤í–‰
            model = YOLO(f"{self.model_size}.pt")
            
            try:
                results = model.train(
                    data=str(self.data_yaml),
                    epochs=10,  # ì§§ì€ í•™ìŠµ
                    batch=params['batch'],
                    lr0=params['lr0'],
                    weight_decay=params['weight_decay'],
                    momentum=params['momentum'],
                    warmup_epochs=params['warmup_epochs'],
                    device=self.device,
                    verbose=False,
                    plots=False,
                    save=False
                )
                
                # mAP@0.5ë¥¼ ìµœì í™” ëª©í‘œë¡œ ì‚¬ìš©
                return results.results_dict.get('metrics/mAP50(B)', 0.0)
                
            except Exception as e:
                print(f"Trial ì‹¤íŒ¨: {e}")
                return 0.0
        
        # Optuna ìŠ¤í„°ë”” ì‹¤í–‰
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials, timeout=timeout)
        
        best_params = study.best_params
        
        print(f"ğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in best_params.items():
            print(f"   {key}: {value}")
        
        return best_params
    
    def _evaluate_model(self, model_path: str, data_yaml: str) -> float:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (mAP@0.5 ë°˜í™˜)"""
        try:
            model = YOLO(model_path)
            results = model.val(data=data_yaml, device=self.device, verbose=False)
            return results.results_dict.get('metrics/mAP50(B)', 0.0)
        except Exception:
            return 0.0
    
    def create_backup_strategy_plan(self) -> Dict:
        """ë°±ì—… ì „ëµ ê³„íš ìƒì„±"""
        strategies = {
            'primary': {
                'model': self.model_size,
                'strategy': 'Progressive Class Expansion',
                'expected_performance': 0.80,
                'training_time_days': 12,
                'gpu_requirement_gb': 8
            },
            'backup_level_1': {
                'model': 'yolov8n',
                'strategy': 'All-class simultaneous training',
                'expected_performance': 0.70,
                'training_time_days': 6,
                'gpu_requirement_gb': 4
            },
            'backup_level_2': {
                'model': 'yolov8s',  
                'strategy': 'Traditional training with reduced epochs',
                'expected_performance': 0.65,
                'training_time_days': 3,
                'gpu_requirement_gb': 6
            }
        }
        
        return strategies


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="YOLOv8 ê³ ê¸‰ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--data", required=True, help="ë°ì´í„°ì…‹ YAML íŒŒì¼")
    parser.add_argument("--model", default="yolov8s", help="ëª¨ë¸ í¬ê¸°")
    parser.add_argument("--epochs", type=int, default=100, help="í•™ìŠµ ì—í­ ìˆ˜")
    parser.add_argument("--batch", default="auto", help="ë°°ì¹˜ í¬ê¸°", 
                        type=lambda x: int(x) if x.isdigit() else x)
    parser.add_argument("--imgsz", type=int, default=2048, help="ì´ë¯¸ì§€ í¬ê¸°")
    parser.add_argument("--patience", type=int, default=15, help="ì¡°ê¸° ì¢…ë£Œ patience")
    parser.add_argument("--name", help="ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--progressive", action="store_true", help="ì ì§„ì  í•™ìŠµ ì‹¤í–‰")
    parser.add_argument("--optimize", action="store_true", help="í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
    parser.add_argument("--no-wandb", action="store_true", help="W&B ë¹„í™œì„±í™”")
    
    args = parser.parse_args()
    
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    trainer = YOLOTrainer(
        data_yaml=args.data,
        model_size=args.model,
        use_wandb=not args.no_wandb
    )
    
    try:
        if args.optimize:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            best_params = trainer.optimize_hyperparameters()
            
            # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ ì‹¤í–‰
            print("\nğŸš€ ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ ì‹œì‘...")
            # ì—¬ê¸°ì„œ best_paramsë¥¼ í™œìš©í•œ í•™ìŠµ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            
        elif args.progressive:
            # ì ì§„ì  í•™ìŠµ
            phases = {
                'phase_1': {
                    'classes': ['noteheadFull', 'stem', 'gClef'],
                    'epochs': 50,
                    'target_mAP': 0.85,
                    'batch': 8
                },
                'phase_2': {
                    'classes': ['restQuarter', 'beam', 'dot'],
                    'epochs': 30,
                    'target_mAP': 0.80,
                    'batch': 8
                },
                'phase_3': {
                    'classes': ['sharp', 'flat', 'natural'],
                    'epochs': 20,
                    'target_mAP': 0.75,
                    'batch': 8
                }
            }
            
            phase_models = trainer.progressive_training(phases)
            
            print(f"\nğŸ‰ ì ì§„ì  í•™ìŠµ ì™„ë£Œ!")
            for phase, model_path in phase_models.items():
                print(f"   {phase}: {model_path}")
                
        else:
            # ì¼ë°˜ í•™ìŠµ
            best_model = trainer.train(
                epochs=args.epochs,
                batch=args.batch,
                imgsz=args.imgsz,
                patience=args.patience,
                name=args.name
            )
            
            print(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ: {best_model}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        if trainer.use_wandb:
            wandb.finish()


if __name__ == "__main__":
    main()