#!/usr/bin/env python3
"""
YOLOv8 ê¸°ë°˜ ì•…ë³´ ê¸°í˜¸ íƒì§€ ì‹œìŠ¤í…œ
ScoreEyeì—ì„œ ì¶”ì¶œëœ ë§ˆë”” ì´ë¯¸ì§€ì—ì„œ ê°œë³„ ìŒì•… ê¸°í˜¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import argparse
import json

import numpy as np
import cv2
from ultralytics import YOLO
import torch


class SymbolDetector:
    """YOLOv8 ê¸°ë°˜ ìŒì•… ê¸°í˜¸ ê°ì§€ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, conf_threshold: float = 0.5, device: str = 'auto'):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ YOLOv8 ëª¨ë¸ ê²½ë¡œ (.pt íŒŒì¼)
            conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda', 'mps')
        """
        self.model_path = Path(model_path)
        self.conf_threshold = conf_threshold
        
        if not self.model_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        # ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
        if device == 'auto':
            if torch.cuda.is_available():
                device = 'cuda'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = 'mps'
            else:
                device = 'cpu'
        
        self.device = device
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {self.device})")
        self.model = YOLO(str(model_path))
        self.model.to(self.device)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        self.class_names = self.model.names
        print(f"ğŸ“‹ ê°ì§€ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ìˆ˜: {len(self.class_names)}")
        for class_id, class_name in self.class_names.items():
            print(f"   {class_id}: {class_name}")
    
    def detect(self, 
               image_input: Union[str, np.ndarray], 
               return_format: str = 'dict',
               save_visualization: Optional[str] = None) -> List[Dict]:
        """
        ì´ë¯¸ì§€ì—ì„œ ì•…ë³´ ê¸°í˜¸ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
        
        Args:
            image_input: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” numpy ë°°ì—´
            return_format: ë°˜í™˜ í˜•ì‹ ('dict', 'yolo_format')  
            save_visualization: ì‹œê°í™” ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            
        Returns:
            ê°ì§€ëœ ê¸°í˜¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸. ê° í•­ëª©ì€ ë‹¤ìŒì„ í¬í•¨:
            - box: (x1, y1, x2, y2) ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
            - class_name: í´ë˜ìŠ¤ ì´ë¦„
            - class_id: í´ë˜ìŠ¤ ID
            - confidence: ì‹ ë¢°ë„ ì ìˆ˜
            - center: (x_center, y_center) ì¤‘ì‹¬ì  ì¢Œí‘œ
        """
        start_time = time.time()
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image_input, str):
            if not Path(image_input).exists():
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
            image = cv2.imread(str(image_input))
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
        else:
            image = image_input.copy()
        
        # ì¶”ë¡  ì‹¤í–‰
        results = self.model(image, conf=self.conf_threshold, verbose=False)
        
        detected_symbols = []
        
        for r in results:
            if r.boxes is not None:
                boxes = r.boxes
                
                for box in boxes:
                    # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                    
                    # ì‹ ë¢°ë„ì™€ í´ë˜ìŠ¤
                    conf = float(box.conf[0].cpu().numpy())
                    cls_id = int(box.cls[0].cpu().numpy())
                    class_name = self.class_names.get(cls_id, f"unknown_{cls_id}")
                    
                    # ì¤‘ì‹¬ì  ê³„ì‚°
                    x_center = (x1 + x2) // 2
                    y_center = (y1 + y2) // 2
                    
                    symbol_data = {
                        'box': (x1, y1, x2, y2),
                        'class_name': class_name,
                        'class_id': cls_id,
                        'confidence': conf,
                        'center': (x_center, y_center),
                        'width': x2 - x1,
                        'height': y2 - y1
                    }
                    
                    detected_symbols.append(symbol_data)
        
        # ì‹ ë¢°ë„ìˆœìœ¼ë¡œ ì •ë ¬
        detected_symbols.sort(key=lambda x: x['confidence'], reverse=True)
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = time.time() - start_time
        
        # ì‹œê°í™” ì €ì¥
        if save_visualization:
            self._save_visualization(image, detected_symbols, save_visualization)
        
        # ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
        print(f"âš¡ ê°ì§€ ì™„ë£Œ: {len(detected_symbols)}ê°œ ê¸°í˜¸ ({processing_time:.3f}ì´ˆ)")
        
        return detected_symbols
    
    def detect_batch(self, 
                     image_paths: List[str], 
                     output_dir: Optional[str] = None,
                     save_visualizations: bool = False) -> Dict[str, List[Dict]]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•´ ë°°ì¹˜ ê°ì§€ ìˆ˜í–‰
        
        Args:
            image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬
            save_visualizations: ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€
            
        Returns:
            ê° ì´ë¯¸ì§€ë³„ ê°ì§€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        print(f"ğŸ“¦ ë°°ì¹˜ ê°ì§€ ì‹œì‘: {len(image_paths)}ê°œ ì´ë¯¸ì§€")
        
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            if save_visualizations:
                vis_dir = output_dir / "visualizations"
                vis_dir.mkdir(exist_ok=True)
        
        for i, image_path in enumerate(image_paths, 1):
            image_path = Path(image_path)
            print(f"[{i}/{len(image_paths)}] ì²˜ë¦¬ ì¤‘: {image_path.name}")
            
            try:
                # ì‹œê°í™” ê²½ë¡œ ì„¤ì •
                vis_path = None
                if save_visualizations and output_dir:
                    vis_path = vis_dir / f"{image_path.stem}_detected.jpg"
                
                # ê°ì§€ ì‹¤í–‰
                detected = self.detect(str(image_path), save_visualization=vis_path)
                results[str(image_path)] = detected
                
                # JSON ê²°ê³¼ ì €ì¥
                if output_dir:
                    json_path = output_dir / f"{image_path.stem}_detections.json"
                    with open(json_path, 'w') as f:
                        json.dump(detected, f, indent=2, ensure_ascii=False)
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {str(e)}")
                results[str(image_path)] = []
        
        print(f"âœ… ë°°ì¹˜ ê°ì§€ ì™„ë£Œ!")
        return results
    
    def tiled_inference(self, 
                       image: np.ndarray, 
                       tile_size: int = 1024, 
                       overlap: int = 128) -> List[Dict]:
        """
        í° ì´ë¯¸ì§€ë¥¼ íƒ€ì¼ë¡œ ë‚˜ëˆ„ì–´ ì¶”ë¡ í•˜ê³  ê²°ê³¼ë¥¼ ë³‘í•©
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ (numpy ë°°ì—´)
            tile_size: íƒ€ì¼ í¬ê¸° (ì •ì‚¬ê°í˜•)
            overlap: íƒ€ì¼ ê°„ ê²¹ì¹¨ í¬ê¸°
            
        Returns:
            ì „ì²´ ì´ë¯¸ì§€ì—ì„œì˜ ê°ì§€ ê²°ê³¼
        """
        h, w = image.shape[:2]
        
        # íƒ€ì¼ì´ í•„ìš”ì—†ì„ ì •ë„ë¡œ ì‘ì€ ì´ë¯¸ì§€ë©´ ì¼ë°˜ ì¶”ë¡ 
        if max(h, w) <= tile_size:
            return self.detect(image)
        
        print(f"ğŸ”² Tiled Inference: {w}x{h} â†’ {tile_size}x{tile_size} íƒ€ì¼")
        
        step = tile_size - overlap
        all_detections = []
        
        for y in range(0, h - tile_size + 1, step):
            for x in range(0, w - tile_size + 1, step):
                # íƒ€ì¼ ì¶”ì¶œ
                tile = image[y:y+tile_size, x:x+tile_size]
                
                # íƒ€ì¼ì—ì„œ ê°ì§€ ì‹¤í–‰
                tile_detections = self.detect(tile)
                
                # ì „ì²´ ì´ë¯¸ì§€ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                for detection in tile_detections:
                    x1, y1, x2, y2 = detection['box']
                    detection['box'] = (x1+x, y1+y, x2+x, y2+y)
                    detection['center'] = (detection['center'][0]+x, detection['center'][1]+y)
                
                all_detections.extend(tile_detections)
        
        # ê²½ê³„ ì˜ì—­ ì²˜ë¦¬ (ì˜¤ë¥¸ìª½ ë° ì•„ë˜ìª½ ë)
        if w % step != 0:
            for y in range(0, h - tile_size + 1, step):
                x = w - tile_size
                tile = image[y:y+tile_size, x:x+tile_size]
                tile_detections = self.detect(tile)
                
                for detection in tile_detections:
                    x1, y1, x2, y2 = detection['box']
                    detection['box'] = (x1+x, y1+y, x2+x, y2+y)
                    detection['center'] = (detection['center'][0]+x, detection['center'][1]+y)
                
                all_detections.extend(tile_detections)
        
        if h % step != 0:
            y = h - tile_size
            for x in range(0, w - tile_size + 1, step):
                tile = image[y:y+tile_size, x:x+tile_size]
                tile_detections = self.detect(tile)
                
                for detection in tile_detections:
                    x1, y1, x2, y2 = detection['box']
                    detection['box'] = (x1+x, y1+y, x2+x, y2+y)
                    detection['center'] = (detection['center'][0]+x, detection['center'][1]+y)
                
                all_detections.extend(tile_detections)
        
        # ì¤‘ë³µ ì œê±° (NMS ìœ ì‚¬í•œ ë°©ì‹)
        final_detections = self._remove_duplicate_detections(all_detections)
        
        print(f"   íƒ€ì¼ ê°ì§€: {len(all_detections)}ê°œ â†’ ì¤‘ë³µ ì œê±° í›„: {len(final_detections)}ê°œ")
        
        return final_detections
    
    def _remove_duplicate_detections(self, detections: List[Dict], iou_threshold: float = 0.5) -> List[Dict]:
        """ì¤‘ë³µ ê°ì§€ ê²°ê³¼ ì œê±° (NMS ë°©ì‹)"""
        if len(detections) <= 1:
            return detections
        
        # ì‹ ë¢°ë„ìˆœìœ¼ë¡œ ì •ë ¬
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        
        keep = []
        
        while detections:
            current = detections.pop(0)
            keep.append(current)
            
            # ë‚¨ì€ detectionë“¤ê³¼ IoU ê³„ì‚°
            remaining = []
            for det in detections:
                iou = self._calculate_iou(current['box'], det['box'])
                
                # ê°™ì€ í´ë˜ìŠ¤ì´ê³  IoUê°€ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ ìœ ì§€
                if current['class_id'] != det['class_id'] or iou < iou_threshold:
                    remaining.append(det)
            
            detections = remaining
        
        return keep
    
    def _calculate_iou(self, box1: Tuple[int, int, int, int], box2: Tuple[int, int, int, int]) -> float:
        """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ê°„ IoU ê³„ì‚°"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        if x2_inter <= x1_inter or y2_inter <= y1_inter:
            return 0.0
        
        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0
    
    def _save_visualization(self, image: np.ndarray, detections: List[Dict], output_path: str):
        """ê°ì§€ ê²°ê³¼ ì‹œê°í™” ì €ì¥"""
        vis_image = image.copy()
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (í´ë˜ìŠ¤ë³„ ë‹¤ë¥¸ ìƒ‰ìƒ)
        colors = [
            (255, 0, 0),    # ë¹¨ê°•
            (0, 255, 0),    # ì´ˆë¡
            (0, 0, 255),    # íŒŒë‘
            (255, 255, 0),  # ë…¸ë‘
            (255, 0, 255),  # ìì£¼
            (0, 255, 255),  # ì²­ë¡
            (128, 0, 128),  # ë³´ë¼
            (255, 165, 0),  # ì£¼í™©
            (0, 128, 0),    # ì–´ë‘ìš´ ì´ˆë¡
            (128, 128, 0),  # ì˜¬ë¦¬ë¸Œ
        ]
        
        for detection in detections:
            x1, y1, x2, y2 = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']
            class_id = detection['class_id']
            
            # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ì„ íƒ
            color = colors[class_id % len(colors)]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)
            
            # ë¼ë²¨ í…ìŠ¤íŠ¸
            label = f"{class_name} {confidence:.2f}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ í¬ê¸° ê³„ì‚°
            (text_width, text_height), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
            )
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
            cv2.rectangle(
                vis_image, 
                (x1, y1 - text_height - baseline - 5), 
                (x1 + text_width, y1), 
                color, -1
            )
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(
                vis_image, label, (x1, y1 - baseline - 3), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1
            )
        
        # ì €ì¥
        cv2.imwrite(str(output_path), vis_image)
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            'model_path': str(self.model_path),
            'device': self.device,
            'class_count': len(self.class_names),
            'class_names': self.class_names,
            'confidence_threshold': self.conf_threshold
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)"""
    parser = argparse.ArgumentParser(description="YOLOv8 ìŒì•… ê¸°í˜¸ ê°ì§€")
    parser.add_argument("--model", required=True, help="YOLOv8 ëª¨ë¸ íŒŒì¼ (.pt)")
    parser.add_argument("--image", help="ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼")
    parser.add_argument("--batch", help="ì´ë¯¸ì§€ ë””ë ‰í„°ë¦¬ (ë°°ì¹˜ ì²˜ë¦¬)")
    parser.add_argument("--output", default="output", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬")
    parser.add_argument("--conf", type=float, default=0.5, help="ì‹ ë¢°ë„ ì„ê³„ê°’")
    parser.add_argument("--device", default="auto", help="ë””ë°”ì´ìŠ¤ (auto/cpu/cuda/mps)")
    parser.add_argument("--tiled", action="store_true", help="Tiled inference ì‚¬ìš©")
    parser.add_argument("--tile-size", type=int, default=1024, help="íƒ€ì¼ í¬ê¸°")
    parser.add_argument("--visualize", action="store_true", help="ì‹œê°í™” ì €ì¥")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    detector = SymbolDetector(args.model, conf_threshold=args.conf, device=args.device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_info = detector.get_model_info()
    print("ğŸ¯ ëª¨ë¸ ì •ë³´:")
    for key, value in model_info.items():
        if key != 'class_names':
            print(f"   {key}: {value}")
    
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        if args.image:
            # ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬
            print(f"\nğŸ–¼ï¸ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬: {args.image}")
            
            image = cv2.imread(args.image)
            if image is None:
                raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.image}")
            
            vis_path = output_dir / "detection_result.jpg" if args.visualize else None
            
            if args.tiled:
                detections = detector.tiled_inference(image, tile_size=args.tile_size)
            else:
                detections = detector.detect(image, save_visualization=vis_path)
            
            # ê²°ê³¼ ì €ì¥
            result_path = output_dir / "detections.json"
            with open(result_path, 'w') as f:
                json.dump(detections, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ ê²°ê³¼ ì €ì¥: {result_path}")
            if vis_path:
                print(f"ğŸ–¼ï¸ ì‹œê°í™” ì €ì¥: {vis_path}")
            
        elif args.batch:
            # ë°°ì¹˜ ì²˜ë¦¬
            batch_dir = Path(args.batch)
            if not batch_dir.exists():
                raise FileNotFoundError(f"ë°°ì¹˜ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {batch_dir}")
            
            image_files = []
            for ext in ['*.png', '*.jpg', '*.jpeg']:
                image_files.extend(batch_dir.glob(ext))
            
            if not image_files:
                raise ValueError(f"ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {batch_dir}")
            
            print(f"\nğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬: {len(image_files)}ê°œ íŒŒì¼")
            
            results = detector.detect_batch(
                [str(f) for f in image_files],
                output_dir=output_dir,
                save_visualizations=args.visualize
            )
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            total_detections = sum(len(detections) for detections in results.values())
            print(f"ğŸ“Š ì „ì²´ ê°ì§€ëœ ê¸°í˜¸ ìˆ˜: {total_detections}")
            
        else:
            print("âŒ --image ë˜ëŠ” --batch ì˜µì…˜ì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()