#!/usr/bin/env python3
"""
ScoreEye ë§ˆë”” ì´ë¯¸ì§€ ìŠ¤ì¼€ì¼ ìµœì í™” í…ŒìŠ¤íŠ¸
0.1ë¶€í„° 1.0ê¹Œì§€ 0.1 ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸
"""

import cv2
import numpy as np
from pathlib import Path
from hybrid_music_detector import HybridMusicDetector
import json

def preprocess_with_scale(image_path: str, scale_factor: float, target_size: int = 2048) -> np.ndarray:
    """íŠ¹ì • ìŠ¤ì¼€ì¼ë¡œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    # ìŠ¤ì¼€ì¼ ì ìš©
    height, width = image.shape[:2]
    new_height = int(height * scale_factor)
    new_width = int(width * scale_factor)
    
    if scale_factor < 1.0:
        # ì¶•ì†Œ
        scaled = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
    else:
        # í™•ëŒ€ (1.0ì€ ì›ë³¸)
        scaled = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
    
    # 2048x2048 í•˜ì–€ìƒ‰ ìº”ë²„ìŠ¤ì— ì¤‘ì•™ ë°°ì¹˜
    canvas = np.ones((target_size, target_size, 3), dtype=np.uint8) * 255
    start_x = (target_size - new_width) // 2
    start_y = (target_size - new_height) // 2
    
    end_x = min(start_x + new_width, target_size)
    end_y = min(start_y + new_height, target_size)
    
    crop_width = end_x - start_x
    crop_height = end_y - start_y
    
    canvas[start_y:end_y, start_x:end_x] = scaled[:crop_height, :crop_width]
    
    return canvas

def test_scale_optimization(image_path: str):
    """0.1ë¶€í„° 1.0ê¹Œì§€ ìŠ¤ì¼€ì¼ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    print("=== ScoreEye ìŠ¤ì¼€ì¼ ìµœì í™” í…ŒìŠ¤íŠ¸ ===\\n")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê°ì§€ê¸° ì´ˆê¸°í™”
    model_path = "scoreeye-yolov8/stem_fixed_2048_batch22/weights/best.pt"
    detector = HybridMusicDetector(model_path, confidence_threshold=0.3)
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = []
    
    # 0.3ë¶€í„° 1.0ê¹Œì§€ 0.1 ë‹¨ìœ„ë¡œ í…ŒìŠ¤íŠ¸
    for i in range(3, 11):  # 3ë¶€í„° 10ê¹Œì§€ (0.3ë¶€í„° 1.0)
        scale = i * 0.1
        
        print(f"\\n--- ìŠ¤ì¼€ì¼ {scale:.1f} í…ŒìŠ¤íŠ¸ ---")
        
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            preprocessed = preprocess_with_scale(image_path, scale)
            
            # ì›ë³¸ í¬ê¸° ê³„ì‚°
            original = cv2.imread(image_path)
            orig_h, orig_w = original.shape[:2]
            scaled_h, scaled_w = int(orig_h * scale), int(orig_w * scale)
            
            print(f"ì›ë³¸: {orig_h}Ã—{orig_w} â†’ ìŠ¤ì¼€ì¼ë§: {scaled_h}Ã—{scaled_w}")
            
            # ê°ì§€ ì‹¤í–‰
            yolo_detections, stems = detector.detect(preprocessed)
            
            # í´ë˜ìŠ¤ë³„ ì¹´ìš´íŠ¸
            class_counts = {}
            total_confidence = 0
            max_confidence = 0
            
            for det in yolo_detections:
                class_counts[det.class_name] = class_counts.get(det.class_name, 0) + 1
                total_confidence += det.confidence
                max_confidence = max(max_confidence, det.confidence)
            
            if stems:
                class_counts['stem'] = len(stems)
            
            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            avg_confidence = total_confidence / len(yolo_detections) if yolo_detections else 0
            
            # ê²°ê³¼ ì¶œë ¥
            total_detections = len(yolo_detections) + len(stems)
            print(f"ì´ ê°ì§€: {total_detections}ê°œ")
            print(f"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
            print(f"ìµœê³  ì‹ ë¢°ë„: {max_confidence:.3f}")
            
            if class_counts:
                print("ê°ì§€ëœ í´ë˜ìŠ¤:")
                for class_name, count in sorted(class_counts.items()):
                    print(f"  {class_name}: {count}ê°œ")
            else:
                print("ê°ì§€ëœ ê¸°í˜¸ ì—†ìŒ")
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'scale': scale,
                'scaled_size': f"{scaled_h}Ã—{scaled_w}",
                'total_detections': total_detections,
                'yolo_detections': len(yolo_detections),
                'stem_detections': len(stems),
                'avg_confidence': round(avg_confidence, 3),
                'max_confidence': round(max_confidence, 3),
                'class_counts': class_counts,
                'unique_classes': len(class_counts)
            }
            results.append(result)
            
            # ì‹œê°í™” ì €ì¥ (ìµœê³  ì„±ëŠ¥ ëª‡ ê°œë§Œ)
            if total_detections > 0 and (scale in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]):
                vis_output = f"scale_{scale:.1f}_detection.png"
                detector.visualize_detections(preprocessed, yolo_detections, stems, vis_output)
                print(f"ì‹œê°í™” ì €ì¥: {vis_output}")
            
        except Exception as e:
            print(f"ìŠ¤ì¼€ì¼ {scale:.1f} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results.append({
                'scale': scale,
                'error': str(e)
            })
    
    # ìµœì¢… ê²°ê³¼ ë¶„ì„
    print("\\n\\n=== ì „ì²´ ê²°ê³¼ ìš”ì•½ ===")
    print("ìŠ¤ì¼€ì¼ | í¬ê¸°       | ì´ê°ì§€ | YOLO | Stem | í‰ê· ì‹ ë¢°ë„ | ìµœê³ ì‹ ë¢°ë„ | í´ë˜ìŠ¤ìˆ˜")
    print("-" * 85)
    
    valid_results = [r for r in results if 'error' not in r]
    
    for result in valid_results:
        scale = result['scale']
        size = result['scaled_size']
        total = result['total_detections']
        yolo = result['yolo_detections']
        stem = result['stem_detections']
        avg_conf = result['avg_confidence']
        max_conf = result['max_confidence']
        classes = result['unique_classes']
        
        print(f"{scale:5.1f} | {size:10s} | {total:6d} | {yolo:4d} | {stem:4d} | {avg_conf:8.3f} | {max_conf:8.3f} | {classes:6d}")
    
    # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
    if valid_results:
        # ë‹¤ì–‘í•œ ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        best_total = max(valid_results, key=lambda x: x['total_detections'])
        best_confidence = max(valid_results, key=lambda x: x['avg_confidence'])
        best_variety = max(valid_results, key=lambda x: x['unique_classes'])
        
        print(f"\\n=== ìµœê³  ì„±ëŠ¥ ë¶„ì„ ===")
        print(f"ìµœë‹¤ ê°ì§€: ìŠ¤ì¼€ì¼ {best_total['scale']:.1f} ({best_total['total_detections']}ê°œ)")
        print(f"ìµœê³  ì‹ ë¢°ë„: ìŠ¤ì¼€ì¼ {best_confidence['scale']:.1f} ({best_confidence['avg_confidence']:.3f})")
        print(f"ìµœë‹¤ í´ë˜ìŠ¤: ìŠ¤ì¼€ì¼ {best_variety['scale']:.1f} ({best_variety['unique_classes']}ê°œ)")
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”ëœ ê°€ì¤‘ í•©)
        max_detections = max(r['total_detections'] for r in valid_results)
        max_conf = max(r['avg_confidence'] for r in valid_results)
        max_classes = max(r['unique_classes'] for r in valid_results)
        
        for result in valid_results:
            # 0-1ë¡œ ì •ê·œí™”
            norm_detections = result['total_detections'] / max_detections if max_detections > 0 else 0
            norm_confidence = result['avg_confidence'] / max_conf if max_conf > 0 else 0
            norm_classes = result['unique_classes'] / max_classes if max_classes > 0 else 0
            
            # ê°€ì¤‘ í•© (ê°ì§€ìˆ˜ 40%, ì‹ ë¢°ë„ 40%, ë‹¤ì–‘ì„± 20%)
            composite_score = norm_detections * 0.4 + norm_confidence * 0.4 + norm_classes * 0.2
            result['composite_score'] = round(composite_score, 3)
        
        best_overall = max(valid_results, key=lambda x: x['composite_score'])
        print(f"\\nğŸ¯ ì¢…í•© ìµœì  ìŠ¤ì¼€ì¼: {best_overall['scale']:.1f} (ì¢…í•©ì ìˆ˜: {best_overall['composite_score']:.3f})")
    
    # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    with open('scale_optimization_results.json', 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\\nìƒì„¸ ê²°ê³¼ ì €ì¥: scale_optimization_results.json")
    
    return results

if __name__ == "__main__":
    image_path = "../output/page_01/P1_05_001.png"
    
    try:
        results = test_scale_optimization(image_path)
        print("\\nâœ… ìŠ¤ì¼€ì¼ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()